---
layout: page
title: AI for Science | 研究内容
header_title: 研究内容
---
{% include navigation-research.html %}

<!-- page-main -->
<section id="page-main" class="researchBox padding-small">
  <div class="container">
    <div class="row">
      <div class="col-lg-9">
        <h2>AI for Science</h2>
        <p>AI for Scienceとは文字通り，科学研究のためのAI研究です． その中でも特に科学技術機械学習(scientific machine learning)に取り組んでおり，解析力学や微分幾何学，それらを基にした科学技術計算の技術と機械学習を融合します． データからの高精度な力学系のモデル化，それを用いた計算機シミュレーションにおける物理現象の保証と高速化，データからの物理法則の発見などを可能にします．</p>
      </div>
      <div class="col-lg-3"><img src="/assets/images/research01.jpg" class="img-fluid" width="" height="" alt="AI for Science"></div>
    </div>
    <h3 class="mt-5 pb-3 border-bottom">科学技術機械学習</h3>
    <!-- Card START -->
    <div class="card card-img-scale py-3">
      <div class="row">
        <div class="col-md-3">
          <!-- Card Image -->
          <div class="card-img-scale-wrapper">
            <!-- Image -->
            <a href="https://tksmatsubara.github.io/ja/projects/matsubara2022/" target="_blank"><img src="matsubara2022/thumbnail.png" alt=""></a>
          </div>
        </div>
        <div class="col-md-9">
          <!-- Card body -->
          <div class="card-body">
            <!-- Title -->
            <h4 class="card-title mb-3">物理現象の保存量を発見するニューラル微分方程式</h4>
            <p>データから物理現象をモデル化し，シミュレーションや制御を高性能化する機械学習手法が古くから検討されてきた．特にニューラル常微分方程式 (NODE) は，ニューラルネットワークと数値積分を組み合わせることで，連続時間のダイナミクスの精緻なモデル化に成功した. また，実世界のシステムには，時間の経過とともに変化しない量である「保存量」が存在する ．既知の保存量に関する事前知識を取り入れることで，対象システムを正確に学習することが試みられてきた．例えばハミルトニアンニューラルネットワーク (HNN) は，ハミルトン方程式を近似することで，システムのエネルギーを保存する．この他にも，運動量やホロノミック拘束を取り入れる手法もある．<br>
              しかし，ニューラルネットワークが未知の対象システムを学習する状況では，対象システムの保存量も未知であることが予想される．そこで，データから保存量を発見できる方法として，第一積分保存ニューラル微分方程式 (FINDE) を提案する． FIND は，データから様々な種類の保存量を統一的な枠組みで発見し，与えられたシステムの未来の状態をより長く正確に予測できる．</p>
            <ul>
              <li>Takashi Matsubara and Takaharu Yaguchi, &ldquo;FINDE: Neural Differential Equations for Finding and Preserving Invariant Quantities,&rdquo; <em>Proc. of The Eleventh International Conference on Learning Representations (ICLR2023)</em>, May 2023. (<a href="https://openreview.net/forum?id=tLScKVhcCR">openreview</a>) (<a href="https://arxiv.org/abs/2210.00272">arXiv</a>)</li>
            </ul>
            <p>
              <a href="#" class="btn btn-default btn-tag disabled">Paper<i class="fas fa-file-alt"></i></a>
              <a href="#" class="btn btn-default btn-tag">Slide<i class="fas fa-clone"></i></a>
              <a href="#" class="btn btn-default btn-tag">Poster<i class="fas fa-images"></i></a>
              <a href="#" class="btn btn-default btn-tag">Movie<i class="fas fa-video"></i></a>
              <a href="https://tksmatsubara.github.io/ja/projects/matsubara2022/" target="_blank" class="btn btn-default btn-tag">Link<i class="fas fa-external-link-alt"></i></a>
            </p>
          </div>
        </div>
      </div>
    </div>
    <!-- Card END -->
    <div class="card card-img-scale py-3">
      <div class="row">
        <div class="col-md-3">
          <div class="card-img-scale-wrapper">
            <!-- Card Image -->
            <a href="matsubara2021/index.html"><img src="matsubara2021/0004.png" alt=""></a> </div>
        </div>
        <div class="col-md-9">
          <!-- Card body -->
          <div class="card-body">
            <!-- Title -->
            <h4 class="card-title mb-3">シンプレクティック随伴変数法による高速省メモリなNeural ODEの勾配計算</h4>
            <p>ニューラルネットワークで微分方程式(ODE)を学習するneural ODEは，連続時間のダイナミカルシステムや確率分布を，高い精度でモデル化できます．しかし，同じニューラルネットワークを何度も使うため，誤差逆伝播法で訓練するには非常に大きなメモリが必要になります．そのため数値積分で勾配を計算する随伴変数(アジョイント)法が用いられますが，数値誤差を抑えるために今度は大きな計算コストが必要になります．</p>
            <p>本研究では随伴変数法に用いる数値積分法をシンプレクティックにすることにより，数値誤差なく勾配が計算できる手法(Sanz-Serna, 2016)をベースに，適切なチェックポイント法を組み合わせることで，省メモリ性と高速な勾配計算を両立させました．</p>
            <p><a href="https://www.dropbox.com/s/4bjax7ht0fx88g4/58_%E6%9D%BE%E5%8E%9F%E5%B4%87.mp4?dl=0" target="_blank">IBISワークショップで予定の動画はこちら．</a></p>
            <ul>
              <li>Takashi Matsubara, Yuto Miyatake, and Takaharu Yaguchi, &ldquo;Symplectic Adjoint Method for Exact Gradient of Neural ODE with Minimal Memory,&rdquo; <em>Advances in Neural Information Processing Systems (NeurIPS2021)</em>, 2021. (acceptance rate 26%) (<a href="https://openreview.net/forum?id=46J_l-cpc1W">openreview</a>) (<a href="https://arxiv.org/abs/2102.09750">arXiv</a>)</li>
              <li>Takashi Matsubara, Yuto Miyatake, and Takaharu Yaguchi, &ldquo;The Symplectic Adjoint Method: Memory-Efficient Backpropagation of Neural-Network-Based Differential Equations,&rdquo; <em>IEEE Transactions on Neural Networks and Learning Systems</em>, 2023. (<a href="https://ieeexplore.ieee.org/document/10045756">link</a>)</li>
            </ul>
            <p>
              <a href="#" class="btn btn-default btn-tag">Paper<i class="fas fa-file-alt"></i></a>
              <a href="#" class="btn btn-default btn-tag">Slide<i class="fas fa-clone"></i></a>
              <a href="#" class="btn btn-default btn-tag">Poster<i class="fas fa-images"></i></a>
              <a href="#" class="btn btn-default btn-tag">Movie<i class="fas fa-video"></i></a>
              <a href="#" class="btn btn-default btn-tag">Link<i class="fas fa-external-link-alt"></i></a>
            </p>
          </div>
        </div>
        <!-- Card END -->
      </div>
    </div>
    <div class="card card-img-scale py-3">
      <div class="row">
        <div class="col-md-3">
          <div class="card-img-scale-wrapper">
            <!-- Card Image -->
            <a href="matsubara2020/index.html"><img src="matsubara2020/thumbnail.png" alt=""></a> </div>
        </div>
        <div class="col-md-9">
          <!-- Card body -->
          <div class="card-body">
            <!-- Title -->
            <h4 class="card-title">自動離散微分を用いたエネルギー保存則・散逸則を満たす深層物理シミュレーション</h4>
            <p>機械学習で物理現象をモデル化することは，高速なシミュレーションや未知の現象の発見などに繋がると期待されています．しかし離散時間で行われるシミュレーションでは，エネルギー保存則・散逸則といった物理法則が成り立たず，結果が信頼できなかったり，シミュレーションが破綻したりします．これを回避するため，離散勾配と呼ばれる数値積分の一種が研究されてきましたが，手動での式変形が必要であり，機械学習に適応することは困難でした．</p>
            <p>本発表は自動離散微分という新たなアルゴリズムを提案します．これによって深層学習でデータから対象のダイナミクスを学習し，エネルギーの保存則・散逸則を厳密に保つシミュレーションを行うことが可能になりました．データから学習できるということは，詳細なメカニズムや方程式が未解明の物理現象（波の伝播や結晶構造の成長など）を高い精度でモデル化し，シミュレーションすることが可能になると期待できます．</p>
            <p><a href="https://github.com/tksmatsubara/discrete-autograd" target="_blank">ソースコード公開中．</a></p>
            <p><a href="https://www.dropbox.com/s/flf4wepli2ad7ub/112_%E6%9D%BE%E5%8E%9F%E5%B4%87.mp4?dl=0" target="_blank">IBISワークショップで発表した動画はこちら．</a></p>
            <ul>
              <li>Takashi Matsubara, Ai Ishikawa, and Takaharu Yaguchi, &ldquo;Deep Energy-Based Modeling of Discrete-Time Physics,&rdquo; <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2020. (oral 105/9454=1.1%) (<a href="https://neurips.cc/Conferences/2020/Schedule?showEvent=17224">link</a>) (<a href="https://arxiv.org/abs/1905.08604">arXiv</a>)</li>
            </ul>
            <p>
              <a href="#" class="btn btn-default btn-tag">Paper<i class="fas fa-file-alt"></i></a>
              <a href="#" class="btn btn-default btn-tag">Slide<i class="fas fa-clone"></i></a>
              <a href="#" class="btn btn-default btn-tag">Poster<i class="fas fa-images"></i></a>
              <a href="#" class="btn btn-default btn-tag">Movie<i class="fas fa-video"></i></a>
              <a href="#" class="btn btn-default btn-tag">Link<i class="fas fa-external-link-alt"></i></a>
            </p>
          </div>
        </div>
        <!-- Card END -->
      </div>
    </div>
  </div>
  </div>
</section>
